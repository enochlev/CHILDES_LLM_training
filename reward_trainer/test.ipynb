{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from length_bayesian import BayesianSentenceLengthSkewModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median sentence length: 5.0\n",
      "Text: 'Short.' → Score: 0.6113\n",
      "Text: 'another short one here.' → Score: 0.9817\n",
      "Text: 'This is an average length sentence.' → Score: 0.9997\n",
      "Text: 'This is a very long sentence with many more words than the average training data.' → Score: 0.9650\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/child_response_pairs_scored.csv')\n",
    "\n",
    "df = df.drop_duplicates(subset=['text', 'CHI_response']).reset_index(drop=True)\n",
    "\n",
    "#replace  \\[\\^ [a-z]*\\] with \"\"\n",
    "texts = df['CHI_response'].values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Create and fit model\n",
    "model = BayesianSentenceLengthSkewModel()\n",
    "model.fit(texts)\n",
    "print(f\"Median sentence length: {model.median_length}\")\n",
    "\n",
    "# Make predictions\n",
    "test_texts = [\n",
    "    \"Short.\",\n",
    "    \"another short one here.\",\n",
    "    \"This is an average length sentence.\",\n",
    "    \"This is a very long sentence with many more words than the average training data sldk d d fd ff d f.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    score = model.predict(text,temperature=2)\n",
    "    print(f\"Text: '{text}' → Score: {score:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"../models/bayesian_sentence_length_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherent Examples\n",
      "Score1: 0.7649\tScore2: 0.0052\tScore3: 0.4563\tScore4: -0.3843\tScore5: -0.2534\tScore6: 0.5937 → Avg: 0.8151\n",
      "Score1: 0.7988\tScore2: 0.8243\tScore3: 0.5032\tScore4: -0.3394\tScore5: -0.5773\tScore6: 0.9376 → Avg: 0.8137\n",
      "Score1: 0.7496\tScore2: 0.0073\tScore3: 0.4593\tScore4: 0.9339\tScore5: -0.6467\tScore6: -0.1614 → Avg: 0.2881\n",
      "Score1: 0.8202\tScore2: 0.0024\tScore3: 0.0635\tScore4: -0.3245\tScore5: -0.6100\tScore6: 0.9896 → Avg: 0.5589\n",
      "Score1: 0.7997\tScore2: 0.0101\tScore3: 0.4880\tScore4: 0.1541\tScore5: -0.0321\tScore6: -0.1436 → Avg: 0.6953\n",
      "Score1: 0.8127\tScore2: 0.2658\tScore3: 0.4110\tScore4: -0.5621\tScore5: -0.5026\tScore6: 1.0536 → Avg: 0.7436\n",
      "Score1: 0.7565\tScore2: 0.0027\tScore3: 0.2329\tScore4: -0.1727\tScore5: -0.6176\tScore6: 0.8502 → Avg: 0.7027\n",
      "Score1: 0.7501\tScore2: 0.0626\tScore3: 0.4710\tScore4: -0.6261\tScore5: -0.2497\tScore6: 0.8062 → Avg: 0.7246\n",
      "Score1: 0.8107\tScore2: 0.0022\tScore3: 0.3055\tScore4: 0.7512\tScore5: -0.9373\tScore6: 0.4210 → Avg: 0.5295\n",
      "Score1: 0.7881\tScore2: 0.2249\tScore3: 0.5467\tScore4: -0.7068\tScore5: -0.2640\tScore6: 0.8979 → Avg: 0.7282\n",
      "Bad Non-Coherent Examples\n",
      "Score1: 0.5193\tScore2: 0.0020\tScore3: 0.0107\tScore4: 1.5344\tScore5: -0.9029\tScore6: -0.5040 → Avg: -0.7559\n",
      "Score1: 0.2569\tScore2: 0.0020\tScore3: 0.0097\tScore4: 1.5144\tScore5: -0.8860\tScore6: -0.5070 → Avg: -0.7883\n",
      "Score1: 0.2678\tScore2: 0.0020\tScore3: 0.0100\tScore4: 1.4357\tScore5: -0.8216\tScore6: -0.5190 → Avg: -0.7067\n",
      "Score1: 0.2667\tScore2: 0.0020\tScore3: 0.0105\tScore4: 1.2090\tScore5: -0.7265\tScore6: -0.3945 → Avg: -0.3858\n",
      "Score1: 0.5903\tScore2: 0.0020\tScore3: 0.0094\tScore4: 1.2347\tScore5: -0.7752\tScore6: -0.3429 → Avg: -0.3164\n",
      "Score1: 0.3088\tScore2: 0.0020\tScore3: 0.0539\tScore4: 1.3469\tScore5: -0.8514\tScore6: -0.3546 → Avg: -0.4625\n",
      "Score1: 0.7119\tScore2: 0.0020\tScore3: 0.0090\tScore4: 0.6611\tScore5: -0.5040\tScore6: -0.0777 → Avg: 0.3400\n",
      "Score1: 0.4005\tScore2: 0.0020\tScore3: 0.0097\tScore4: 0.8227\tScore5: -0.5420\tScore6: -0.2231 → Avg: 0.0860\n",
      "Score1: 0.4656\tScore2: 0.0020\tScore3: 0.0103\tScore4: 1.4608\tScore5: -0.8358\tScore6: -0.5273 → Avg: -0.7006\n",
      "Score1: 0.2786\tScore2: 0.0020\tScore3: 0.0100\tScore4: 1.4312\tScore5: -0.7749\tScore6: -0.5946 → Avg: -0.7566\n",
      "Score1: 0.5858\tScore2: 0.0020\tScore3: 0.0092\tScore4: 1.3219\tScore5: -0.7354\tScore6: -0.5138 → Avg: -0.5179\n",
      "Score1: 0.6618\tScore2: 0.0021\tScore3: 0.1295\tScore4: 1.3111\tScore5: -0.7990\tScore6: -0.4089 → Avg: -0.3387\n",
      "Score1: 0.6428\tScore2: 0.0020\tScore3: 0.0101\tScore4: 1.0594\tScore5: -0.6715\tScore6: -0.2912 → Avg: -0.1060\n",
      "Score1: 0.5852\tScore2: 0.0020\tScore3: 0.0100\tScore4: 1.0484\tScore5: -0.6667\tScore6: -0.2768 → Avg: -0.0989\n",
      "Score1: 0.4994\tScore2: 0.0020\tScore3: 0.0096\tScore4: 1.5893\tScore5: -0.8724\tScore6: -0.6294 → Avg: -0.9186\n",
      "Score1: 0.4192\tScore2: 0.0020\tScore3: 0.0096\tScore4: 1.3257\tScore5: -0.8076\tScore6: -0.4001 → Avg: -0.4784\n",
      "Score1: 0.3081\tScore2: 0.0020\tScore3: 0.0096\tScore4: 1.5500\tScore5: -0.8590\tScore6: -0.6151 → Avg: -0.9012\n",
      "Score1: 0.4515\tScore2: 0.0020\tScore3: 0.0091\tScore4: 1.4883\tScore5: -0.8876\tScore6: -0.4635 → Avg: -0.6897\n",
      "Score1: 0.5045\tScore2: 0.0020\tScore3: 0.0113\tScore4: 1.3145\tScore5: -0.7692\tScore6: -0.4562 → Avg: -0.4853\n",
      "Score1: 0.5811\tScore2: 0.0020\tScore3: 0.0093\tScore4: 1.2776\tScore5: -0.7180\tScore6: -0.5008 → Avg: -0.4649\n",
      "Bad Coherent no-context Bad Examples\n",
      "Score1: 0.7660\tScore2: 0.0020\tScore3: 0.2693\tScore4: -0.0144\tScore5: -0.6886\tScore6: 0.7875 → Avg: 0.7313\n",
      "Score1: 0.8129\tScore2: 0.0021\tScore3: 0.2949\tScore4: -0.1633\tScore5: -0.6423\tScore6: 0.8754 → Avg: 0.7272\n",
      "Score1: 0.8078\tScore2: 0.0020\tScore3: 0.0102\tScore4: -0.0418\tScore5: -0.7854\tScore6: 0.9398 → Avg: 0.5315\n",
      "Score1: 0.7358\tScore2: 0.0020\tScore3: 0.3630\tScore4: 1.1777\tScore5: -0.7957\tScore6: -0.2290 → Avg: 0.0090\n",
      "Score1: 0.7836\tScore2: 0.0024\tScore3: 0.2877\tScore4: 0.5125\tScore5: -0.7905\tScore6: 0.4225 → Avg: 0.6997\n",
      "Score1: 0.8363\tScore2: 0.0024\tScore3: 0.1351\tScore4: 0.0127\tScore5: -0.8140\tScore6: 0.9323 → Avg: 0.6212\n",
      "Score1: 0.8011\tScore2: 0.0020\tScore3: 0.2281\tScore4: 0.1996\tScore5: -0.9532\tScore6: 0.9304 → Avg: 0.6294\n",
      "Score1: 0.6977\tScore2: 0.0021\tScore3: 0.2020\tScore4: 0.9224\tScore5: -0.9014\tScore6: 0.2192 → Avg: 0.3178\n",
      "Score1: 0.7741\tScore2: 0.0020\tScore3: 0.1935\tScore4: 0.1990\tScore5: -0.6866\tScore6: 0.5903 → Avg: 0.7226\n",
      "Score1: 0.7375\tScore2: 0.0020\tScore3: 0.1762\tScore4: -0.4045\tScore5: -0.6039\tScore6: 1.0410 → Avg: 0.5786\n",
      "Bad Coherent repeated Bad Examples\n",
      "Score1: 0.7435\tScore2: 0.9709\tScore3: 0.9691\tScore4: -0.7695\tScore5: 0.7868\tScore6: -0.2485 → Avg: 0.1753\n",
      "Score1: 0.6633\tScore2: 0.9581\tScore3: 0.9620\tScore4: -0.6739\tScore5: 0.7231\tScore6: -0.2578 → Avg: 0.2336\n",
      "Score1: 0.7560\tScore2: 0.9651\tScore3: 0.9693\tScore4: -0.8326\tScore5: 0.8123\tScore6: -0.2254 → Avg: 0.1513\n",
      "Score1: 0.6356\tScore2: 0.9619\tScore3: 0.9707\tScore4: -0.7039\tScore5: 0.7236\tScore6: -0.2289 → Avg: 0.2174\n",
      "Score1: 0.7174\tScore2: 0.9604\tScore3: 0.9669\tScore4: -0.7646\tScore5: 0.7800\tScore6: -0.2440 → Avg: 0.1848\n",
      "Score1: 0.6888\tScore2: 0.9692\tScore3: 0.9708\tScore4: -0.7288\tScore5: 0.7487\tScore6: -0.2398 → Avg: 0.1991\n",
      "Score1: 0.6858\tScore2: 0.9652\tScore3: 0.9653\tScore4: -0.8196\tScore5: 0.8085\tScore6: -0.2365 → Avg: 0.1434\n",
      "Score1: 0.7704\tScore2: 0.9560\tScore3: 0.9695\tScore4: -0.8232\tScore5: 0.8297\tScore6: -0.2523 → Avg: 0.1462\n",
      "Score1: 0.7112\tScore2: 0.9316\tScore3: 0.9717\tScore4: -0.8745\tScore5: 0.8698\tScore6: -0.2622 → Avg: 0.0996\n",
      "Score1: 0.7031\tScore2: 0.8590\tScore3: 0.9681\tScore4: -0.9392\tScore5: 0.8471\tScore6: -0.1744 → Avg: 0.1474\n",
      "Score1: 0.7389\tScore2: 0.3452\tScore3: 0.9043\tScore4: -1.0624\tScore5: 0.6230\tScore6: 0.2626 → Avg: 0.3801\n",
      "Score1: 0.7360\tScore2: 0.0178\tScore3: 0.8506\tScore4: -0.5671\tScore5: -0.0464\tScore6: 0.5265 → Avg: 0.6653\n",
      "Score1: 0.7793\tScore2: 0.3924\tScore3: 0.7982\tScore4: -0.8605\tScore5: 0.0273\tScore6: 0.7331 → Avg: 0.6458\n",
      "Score1: 0.7713\tScore2: 0.1272\tScore3: 0.4344\tScore4: 0.7108\tScore5: -0.6364\tScore6: 0.0672 → Avg: 0.6166\n",
      "Score1: 0.7505\tScore2: 0.7482\tScore3: 0.7420\tScore4: -0.7655\tScore5: -0.3059\tScore6: 1.0070 → Avg: 0.6066\n",
      "Score1: 0.6366\tScore2: 0.2513\tScore3: 0.7814\tScore4: -1.0653\tScore5: 0.9477\tScore6: -0.1823 → Avg: 0.2016\n",
      "Score1: 0.6981\tScore2: 0.5410\tScore3: 0.9076\tScore4: -0.8031\tScore5: -0.1461\tScore6: 0.8458 → Avg: 0.5686\n",
      "Score1: 0.7799\tScore2: 0.7288\tScore3: 0.8227\tScore4: -0.7938\tScore5: -0.3296\tScore6: 1.0683 → Avg: 0.5311\n",
      "Score1: 0.7169\tScore2: 0.2782\tScore3: 0.9266\tScore4: -0.9021\tScore5: -0.0648\tScore6: 0.8436 → Avg: 0.4667\n",
      "Score1: 0.6816\tScore2: 0.6012\tScore3: 0.8761\tScore4: -0.9854\tScore5: -0.0544\tScore6: 0.9032 → Avg: 0.4494\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "model_coherence = CrossEncoder(\"../models/child_coherence_model\",device='cuda',max_length=356)#, automodel_args=automodel_args)\n",
    "model_coherence2_2 = CrossEncoder(\"cross-encoder/stsb-roberta-large\",device='cuda',max_length=356)#, automodel_args=automodel_args)\n",
    "model_coherence2 = CrossEncoder(\"cross-encoder/quora-roberta-base\",device='cuda',max_length=356)#, automodel_args=automodel_args)\n",
    "model_coherence3 = CrossEncoder(\"cross-encoder/nli-deberta-v3-base\")#use [:,1] after prediction\n",
    "\n",
    "models = [model_coherence,model_coherence2,model_coherence3]\n",
    "\n",
    "scores = []\n",
    "coherent_examples = [\n",
    "(\"What color is your blue hat?\", \"My hat is blue.\"),\n",
    "(\"Do you want an apple?\", \"Yes, I want an apple.\"),\n",
    "(\"Where is your toy car?\", \"The toy car is here.\"),\n",
    "(\"How old are you?\", \"I am six.\"),\n",
    "(\"Can you count to five?\", \"One two three four five.\"),\n",
    "(\"Are you feeling happy?\", \"Yes, I feel happy.\"),\n",
    "(\"What did you build today?\", \"I built a red tower.\"),\n",
    "(\"Do you want to play now?\", \"I want to play.\"),\n",
    "(\"Where is your book?\", \"My book is on the table.\"),\n",
    "(\"Do you like ice cream?\", \"I love ice cream.\")\n",
    "]\n",
    "\n",
    "random_responses = [\n",
    "(\"Why does the clock hum?\", \"My umbrella sings softly.\"),\n",
    "(\"Is the spoon a spaceship?\", \"My pencil dreams of rainbows.\"),\n",
    "(\"Can you ride the sound?\", \"The book flew away.\"),\n",
    "(\"Where is the silent banana?\", \"My shoes are dancing slow.\"),\n",
    "(\"Do you think the wind eats?\", \"Candy falls from sky.\"),\n",
    "(\"Will the number smile today?\", \"The jelly is laughing loudly.\"),\n",
    "(\"Does the river taste bread?\", \"My water sings in color.\"),\n",
    "(\"Is the car playing chess?\", \"The mirror smiles absurdly.\"),\n",
    "(\"Can chairs whisper secrets?\", \"Balloons argue with trees.\"),\n",
    "(\"Where does the shadow travel?\", \"The cheese forgets its tune.\"),\n",
    "(\"Why is the sky wearing boots?\", \"Cats number the empty stars.\"),\n",
    "(\"Do apples sing to moon?\", \"The clock jumps over apples.\"),\n",
    "(\"Is time a slippery pancake?\", \"My cookie runs on wheels.\"),\n",
    "(\"Can dreams fill the box?\", \"The rain whispers to paper.\"),\n",
    "(\"Do pencils converse softly?\", \"My brother sings underwater.\"),\n",
    "(\"Will the chair jump tonight?\", \"The fruit dreams of dancing.\"),\n",
    "(\"Why do letters feel cold?\", \"The ball giggles at silence.\"),\n",
    "(\"Is the table floating in perfume?\", \"My phone thinks on elephants.\"),\n",
    "(\"Can the pancake swim today?\", \"The radio is a runaway kite.\"),\n",
    "(\"Does music grow on flowers?\", \"My sandwich reads a mystery.\")\n",
    "]\n",
    "\n",
    "hard_random_examples = [\n",
    "(\"What color is the ball?\", \"The ball is on the red bench.\"),\n",
    "(\"Where is your toy?\", \"My toy is beside the sunny porch.\"),\n",
    "(\"Do you want to play?\", \"Yes, but the carpet is slippery.\"),\n",
    "(\"Are you ready for lunch?\", \"Lunch is on the blue sofa.\"),\n",
    "(\"Can you read a story?\", \"I read a story about floating chairs.\"),\n",
    "(\"What did you see outside?\", \"I saw a rainbow near the tall clock.\"),\n",
    "(\"Where is your shoe?\", \"The shoe jumped off the apple tree.\"),\n",
    "(\"Do you like the game?\", \"The game raced past the little boat.\"),\n",
    "(\"Will you wear your jacket?\", \"My jacket is resting at the blue fountain.\"),\n",
    "(\"Are we going home?\", \"Home is next to the green mountain.\"),\n",
    "]\n",
    "repeated_hard_random_examples = [\n",
    "(\"What color is the ball?\", \"What color is the ball?\"),\n",
    "(\"How old are you?\", \"How old are you?\"),\n",
    "(\"Can you count to five?\", \"Can you count to five?\"),\n",
    "(\"Where is your toy?\", \"Where is your toy?\"),\n",
    "(\"Do you want to play now?\", \"Do you want to play now?\"),\n",
    "(\"What did you build today?\", \"What did you build today?\"),\n",
    "(\"Are you feeling happy?\", \"Are you feeling happy?\"),\n",
    "(\"Can you read a story?\", \"Can you read a story?\"),\n",
    "(\"Will you wear your jacket?\", \"Will you wear your jacket?\"),\n",
    "(\"Are we going home?\", \"Are we going home?\"),\n",
    "(\"What color is the ball?\", \"What color do you think is the ball?\"),\n",
    "(\"How old are you?\", \"How old do you think you are?\"),\n",
    "(\"Can you count to five?\", \"Can you count from one to five?\"),\n",
    "(\"Where is your toy?\", \"Your toy is where you left it.\"),\n",
    "(\"Do you want to play now?\", \"Do you want to play with me now?\"),\n",
    "(\"What did you build today?\", \"Did you build something today.\"),\n",
    "(\"Are you feeling happy?\", \"Are you feeling happy today?\"),\n",
    "(\"Can you read a story?\", \"Can you read a story to me?\"),\n",
    "(\"Will you wear your jacket?\", \"Will you wear your jacket today?\"),\n",
    "(\"Are we going home?\", \"Are we going home soon?\")\n",
    "]\n",
    "\n",
    "\n",
    "sentances = coherent_examples + random_responses + hard_random_examples + repeated_hard_random_examples\n",
    "sentances_reversed = [(s[1],s[0]) for s in sentances]\n",
    "#take average of all 3 scores\n",
    "score_1 = model_coherence.predict(sentances)#coherence\n",
    "score_2 = model_coherence2.predict(sentances)#similarity\n",
    "score_3 = model_coherence2_2.predict(sentances)#question similarity\n",
    "score_ = model_coherence3.predict(sentances)# coherence logic\n",
    "score_4 = score_[:,0]/5\n",
    "score_5 = score_[:,1]/5\n",
    "score_6 = score_[:,2]/5\n",
    "\n",
    "final_scores = []\n",
    "import math\n",
    "def logistic(x, midpoint, steepness):\n",
    "    \"\"\"Return logistic function value given midpoint and steepness.\"\"\"\n",
    "    return 1 / (1 + math.exp(-steepness * (x - midpoint)))\n",
    "\n",
    "for i in range(len(sentances)):\n",
    "    # Extract for clarity\n",
    "    s1   = score_1[i]\n",
    "    s2   = score_2[i]\n",
    "    s3  = score_3[i]\n",
    "    s4 = score_4[i]\n",
    "    s5 = score_5[i]\n",
    "    s6 = score_6[i]\n",
    "    #part to modify\n",
    "    # 1) Reward moderate to high s1 (coherence). Let’s just keep it as-is:\n",
    "    t2 = 3.0 * (s2 - s2**2)\n",
    "    t3 = 3.0 * (s3 - s3**2)   # previously 4.0\n",
    "    t6 = 2.0 * (s6 - s6**2)\n",
    "    t1 = 1.0 * s1            # linear in s1\n",
    "\n",
    "    # ——————————————————————————————\n",
    "    # B) Penalties on “extremes”\n",
    "    # ——————————————————————————————\n",
    "    penalty_s4 = -1.0 * (s4**2)\n",
    "    penalty_s5 = -0.5 * (s5**2)\n",
    "\n",
    "    raw = t1 + t2 + t3 + t6 + penalty_s4 + penalty_s5\n",
    "\n",
    "    # ——————————————————————————————\n",
    "    # C) Additional shape rules\n",
    "    # ——————————————————————————————\n",
    "    # 1) If s3 is over 0.7, penalize further:\n",
    "    if s3 > 0.7:\n",
    "        raw -= 2.5 * (s3 - 0.7)**2   # bumped from 2.0 up to 2.5\n",
    "\n",
    "    # 2) If both s2 and s3 are above 0.75, that often means “repeated,”\n",
    "    #    so apply a compact penalty:\n",
    "    if (s2 > 0.75) and (s3 > 0.75):\n",
    "        raw -= 0.8\n",
    "\n",
    "    # 3) Keep the existing s4 checks for non‐coherence:\n",
    "    if s4 > 0.5:\n",
    "        raw -= 2.0 * (s4 - 0.5)\n",
    "    if s4 < -0.5:\n",
    "        raw -= 1.5 * (-0.5 - s4)\n",
    "\n",
    "    # ——————————————————————————————\n",
    "    # D) Final scaling\n",
    "    # ——————————————————————————————\n",
    "    # Shift raw up by +2 and divide by 5, so “good” examples\n",
    "    # tend to be in [0.6..0.8+] and “bad repeated” or “non‐coherent”\n",
    "    # push further down.\n",
    "    avg = (raw + 2.0) / 5.0\n",
    "\n",
    "    #clipped = max(0, min(1, raw))\n",
    "    final_scores.append(avg)\n",
    "    if i == 0:\n",
    "        print(f\"Coherent Examples\")\n",
    "    if i == len(coherent_examples):\n",
    "        print(f\"Bad Non-Coherent Examples\")\n",
    "    if i == len(coherent_examples) + len(random_responses):\n",
    "        print(f\"Bad Coherent no-context Bad Examples\")\n",
    "    if i == len(coherent_examples) + len(random_responses) + len(hard_random_examples):\n",
    "        print(f\"Bad Coherent repeated Bad Examples\")\n",
    "    #print(f\"Text: '{sentances[i][0]}' → '{sentances[i][1]}' → Score1: {score_1[i]:.4f} Score2: {score_2[i]:.4f} Score2_2: {score_2_2[i]:.4f} Score3: {score_3[i]:.4f} → Avg: {avg:.4f}\")\n",
    "    #print(f\"Text: '{sentances[i][0]}' → '{sentances[i][1]}'\")\n",
    "    print(f\"Score1: {score_1[i]:.4f}\\tScore2: {score_2[i]:.4f}\\tScore3: {score_3[i]:.4f}\\tScore4: {score_4[i]:.4f}\\tScore5: {score_5[i]:.4f}\\tScore6: {score_6[i]:.4f} → Avg: {avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03960000000000008"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = .99\n",
    "(i - (i**2))*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.0020019182)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take min of Score2 and Score2_2\n",
    "#if Score 3 is high and Score2 and Score2_2 is also high that is a bad call\n",
    "\n",
    "\n",
    "\n",
    "model_coherence2.predict((\"Can you count to five?\",\"How old are you?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.79409254, 0.7756106 , 0.82671916, 0.836703  ], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/child_response_pairs.csv')\n",
    "df[\"mixed\"] = df[\"child_coherence_score_prediction\"] * 0.7 + predictions * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well, I put it in the garbage.-->> \tI put it in the garbage.-->>\t0.8724676206391143\n",
      "yeah. you threw away the catalogue that Gammy gave me.-->> \tyou threw the catalogue that Gammy gave me.-->>\t0.8721306117636413\n",
      "that's daddy and Gus.-->> \tthat is daddy and Gus.-->>\t0.8720316762402344\n",
      "I'm gonna get me some juice.-->> \tI'm gonna get me some juice.-->>\t0.8682353681704712\n",
      "I'm going to wash my hands.-->> \tI'm going to wash up my hands.-->>\t0.8640208010774993\n",
      "there's Baaee's thumb.-->> \tthere is Baaee's thumb.-->>\t0.863840229026413\n",
      "that's a fishing boat.-->> \ta fishing boat.-->>\t0.8633416848856353\n",
      "the baby's shirt is all wrinkled.-->> \tbaby's, shirt, all, wrinkled.-->>\t0.8633239476805572\n",
      "he has them on his head.-->> \the have them on he head.-->>\t0.8633134659323882\n",
      "that's my comb.-->> \tthat's my comb.-->>\t0.863229350752716\n"
     ]
    }
   ],
   "source": [
    "df_display = df.sort_values(by=['mixed'], ascending=False).head(10)\n",
    "\n",
    "for index, row in df_display.iterrows():\n",
    "    prediction = row[f\"mixed\"]\n",
    "    print(f\"{row['text']}-->> \\t{row['CHI_response']}-->>\\t{prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bsuhome/enochlevandovsky/miniforge3/envs/childes/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|██████████| 517/517 [09:12<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh thank_you.-->> \tI'm gonna buy candy. I'm going to buy some candy.-->>\t4.999378681182861\n",
      "I see.-->> \twe're going to fed her after her nap. gonna feed her after her nap.-->>\t4.999053001403809\n",
      "I want carrot cake, Mom.-->> \tI would like carrot cake, Mommy.-->>\t4.975674629211426\n",
      "0are you sure?-->> \tI think I found something. I think I found.-->>\t4.960228443145752\n",
      "yeah.-->> \tI want go outside. I wanna go outside.-->>\t4.960019111633301\n",
      "okay you guys come here.-->> \tfix your poor dad. come and fix your poor daddy.-->>\t4.955489635467529\n",
      "she likes those though.-->> \tyeah, she does like toys. she does like toys though.-->>\t4.953551292419434\n",
      "he has no love for the Empire.-->> \the has no love for the Empire.-->>\t4.95037317276001\n",
      "oh well.-->> \the hadta he hadta. never he minds. I'll iron your stuff what do you think? I don't mind. I'd iron your.-->>\t4.944336891174316\n",
      "I got it.-->> \tI get it.-->>\t4.942104339599609\n",
      "oh.-->> \tChim Chim Charlie loves karate Chim Chim Charlie love karate.-->>\t4.932529926300049\n",
      "he did went downtown.-->> \the did go downtown.-->>\t4.932143211364746\n",
      "yes. but he was still hungry sweetheart.-->> \the was still hungry.-->>\t4.931365489959717\n",
      "I'm all finished with my yogurt.-->> \tI finish my yogurt.-->>\t4.929259777069092\n",
      "Abe'll be happy.-->> \tAbe will be happy.-->>\t4.925105571746826\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "score_column = 'coherence_score'\n",
    "\n",
    "\n",
    "#model = CrossEncoder('cross-encoder/stsb-roberta-large',max_length=96)\n",
    "model = CrossEncoder(\"cross-encoder/nli-deberta-v3-base\",device='cuda',max_length=96)\n",
    "df = pd.read_csv('../data/child_response_pairs.csv')\n",
    "texts = [[row['text'], row['CHI_response']] for _, row in df.iterrows()]\n",
    "\n",
    "\n",
    "predictions = model.predict(texts, batch_size=1024, show_progress_bar=True)[:,1]\n",
    "df[f\"{score_column}_prediction\"] = predictions\n",
    "#df.to_csv('../data/child_response_pairs.csv', index=False)\n",
    "\n",
    "#print top 10 predictions sorted by score\n",
    "df_display = df.sort_values(by=f\"{score_column}_prediction\", ascending=False).head(15)\n",
    "\n",
    "\n",
    "for index, row in df_display.iterrows():\n",
    "    prediction = row[f\"{score_column}_prediction\"]\n",
    "    print(f\"{row['text']}-->> \\t{row['CHI_response']}-->>\\t{prediction}\")\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(4.9993787)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay.-->> \tso you move your pawn that is in front of your king up. no it is the pawn in front of your king up. and then next turn you take your bishop and put it about I think two two squares so that before it would hit the edge and then your queen all the way over to the edge. and then take your queen and go diagonal. go diagonal and it would it would take the pawn. and then your king would try and get your queen. but your bishop would be there. and it would it would take your king. so then so then you would be in checkmate.-->>\t0.6587844491004944\n",
      "so is there any other strategies for starting the game or?-->> \twell, if the person is like new to the game, the three move one. that is where you put the you move a pawn that is by their by the queen and another bishop. I think it's I am not sure what side it is on. but you move your bishop out so it is like diagonally from like one of the squares from where the queen is going to be. the queen moves out either diagonally. or it goes out diagonally to either the square that is right in front of the I think it's the bishop. and you, and when you if you attack it, when, with since there are no defenses from the king, the king can not attack the queen cause the bishop will put it in check.-->>\t0.6557798981666565\n",
      "started.-->> \tstarted to look for. some food. on. Monday he. ate. through a apple but he was. still hungry. on Tuesday he. ate through. two pears but he was still hungry. on Wednesday he ate through three. plums but he was still hungry. on Thursday he ate through. four strawberries but he was still hungry. on Friday he ate through. five oranges but he was still hungry. on Saturday he ate through one.-->>\t0.6554672122001648\n",
      "can you tell me about it.-->> \tso there is two different clocks. and there is analog. and there is digital. so, the digital ones you can have delay on it so that at the beginning of your. there is two different types of delay. there's at the beginning of your move they give you a certain amount of time, so maybe five seconds. and then your time starts. and then the second type of delay is after your move, they add on another five seconds. so you might have started with five minutes. but if you moved within, a certain like a couple seconds so then they would add time onto it. so then you would have more time than with you started with. and they have it so that as soon as you start your clock if there is no delay then it just keeps running down every second. and if you have thirty minutes then it, once it gets to zero it stops. then the game is over. and the outcome depends on the position. and so you make your move. and then if you are notating, you notate before or after you move. and then you press your clock. but you can only press your side of the clock. and for analog clocks they have it so that you just start the minute hand on one part. and then it just goes around the clock. and then there is a red flag five minutes before the one o'clock, so that the the minute hand will go around. and then it will pass the flag. and when it passes the flag, that is when your time is up.-->>\t0.6520336270332336\n",
      "so tell me about the goals of chess? so in other words what are the players trying to do?-->> \tplayers are trying to get the king into checkmate. and checkmate is when the king is the king is here. and then there are pieces surrounding it and it can't it is it is. and it can't it is it is it has to move so it would not get hit for the next turn. but when it moves in any direction it still gets it is still gonna to get hit. and that is checkmate. and they win.-->>\t0.6487234830856323\n",
      "were there any others that you left out?-->> \twell there is a lot. so in the opening, you try to get all your guys out because you will probably have more space. you attack the center in the opening because the center is where all the pieces tend to point at. and you have to keep your king safe. and the middle game you have to see if your advantages are material or pawn structure. you generally want to go to the end game because those are more important in the end game. and if your advantages are time and space and force then you really want to attack the king because if you go to the end game those really do not matter any more. and so in the end game the strategy is usually to well get all your pieces on their best squares first, get your rook behind a pass pawn or on the seventh rank, get your king in the center of the board or to a good square. well, those are some key strategies.-->>\t0.6480288505554199\n",
      "tell me about the different pieces and how they are supposed to move.-->> \tthere is the pawn. and you can move it two spaces forward on the first try. and and after that it can only move up once. and it can it can take out pieces diagonally one one space. and then the rook can can move any space horizontal or vertical as much as it wants. but it like cannot jump over pieces. and then the knight can. it moves like in an l pattern like three I think up and like one to the left to the right or anything in an l pattern. and it can jump pieces. and the queen can move anywhere like diagonally or vertically or horizontal. and but it can not jump pieces not like the knight just one way. and the king can just move one square anywhere. and the bishop can move any space diagonally on its color.-->>\t0.6463366150856018\n",
      "what makes a game last a long time?-->> \tit is some, well like a ninth of the time. is I am thinking. but then it is just it is going to take a lot of moves to get checkmate because you have got to. it takes like five minutes to get that catch their queen thing developed then another move to catch the queen once the queen got its set. it takes like six moves to do that in the first place. and each time it takes five seconds to think. and that and then you imagine if you have hundreds of plots like that and only half of them work. and then you have to do them over. and then like every time half of them work. it would take a long time.-->>\t0.6446610689163208\n",
      "is that it?-->> \the started to look for some food. on Monday he ate. through a one apple. but he was still hungry. on Tuesday. he ate through two pears but he was still hungry. on Wednesday. he. ate. through. three plums. but he was still hungry. on Thursday he ate through. four strawberries. but he was still hungry. on Friday he ate through five oranges. but he was. still hungry.-->>\t0.642917811870575\n",
      "can you tell me about the different pieces and how they are supposed to move.-->> \tthe pawn well the pawn can move one place at a time. but if it is the first time you move that pawn, you can move it once. so you can move it like up one square or up two squares. but that is the only time that you can move it up two squares. the rook can only move in straight so like up left right down. that is the only way. knights can move up two and side three. and they can hop over pieces. so knights are really special. bishops can only go diagonally. queens are the second to best piece. they can go diagonal or straight. the kings are the most important piece. if you get your king in checkmate then it is over. and the king can only move one square at a time but in any direction. so that is good. if you get a pawn to the other side, then you get to turn it into whatever piece you want it to be. and so like, umm and there are other like moves like the castle. if you have not moved your rook with the king, then you can go like that. like this is the king. and this is the rook. you can go like that. and it is really cool.-->>\t0.6395151019096375\n",
      "yeah.-->> \tand that day. they he went and he and he saw a red balloon. on tied on to a lamppost. and so he ran to the bus stop. after he got the balloon. and he let go of the red balloon at the bus stop. to get to ride to a corner. so he could walk somewhere. so he when he got out at the corner. he walked to the place. that he was going to. and he walked down this alley. in between two buildings. and and then they ran the these bullies. and they ran and ran after him. because they wanted the balloon too.-->>\t0.6374833583831787\n",
      "okay, now tell me about the goals of chess. in other words, what are the players trying to do.-->> \tthe players are trying to checkmate their opponent. so they would so when you check a piece it is like you are going to take it the next turn you get. so you can you are trying to checkmate. but other outcomes can be stalemate, where the opponent can not move any of their pieces without getting themselves in check or making illegal moves. and there is a draw also. it depends on what types of circumstances there are. like, if there is just two kings at the end of the game, you can not stalemate with a, I mean you can not checkmate with a king or stalemate. so each opponent gets half a point. so you are trying to checkmate. but you can get different outcomes.-->>\t0.6374639868736267\n",
      "he ate through.-->> \tand he. through a apple. but he was still hungry. on. Tuesday he ate through. two pears. but he was still hungry. and then. and Wednesday he ate through. three plums. but he was still hungry. on Thursday he ate through. four strawberries. but he was still hungry. on Friday he. ate through. five oranges. but he was still hungry. on Saturday he ate through. one peach of. chocolate cake. one. icecream cone and pickle. and one slice of.-->>\t0.6372942328453064\n",
      "he got.-->> \tand when he was done. going down the steps. he went to the bus stop. and then he let go of the balloon. and then the balloon followed behind the bus. and he was very quick. and the balloon was very quick. and so when he stopped. the balloon came down to the boy again. and then they and then they started walking again. then. they were some other kids saw. and they wanted to get the balloon. and so they and so but so the boy ran. and let the balloon go. and so when he let the balloon go. they all walked up. looking for it. but he could still get it. because it would come back down. and then after they leave. the balloon came back down. and and then the little boy around his house a lot. and then he went to the bakery. left the balloon outside. and then. he got something. that he wanted to get. but then the mean kids came around. and stole the balloon. and they. they took the balloon. and then the boy came out again. and found and then they was running around. and they saw it. and then the big kids were fighting. to get the balloon. and they were using slingshots. to get the other to get the other kids. that wanted it. and and and so and and a few were pulling on a bigger string. which they had put on. and then there was another string. a shorter string. that the boy grabbed on to. and they were both pulling. and then the boy was getting pulled up onto the wall. but then the boy. broke the other string. and then they fell.-->>\t0.6349875330924988\n",
      "the rules.-->> \twell pawns umm go two spaces on their opening move. but they cannot go to the sides. then oh and knights most pieces if it wants to get, if one piece is on one side of the board and another piece is on the other, like the rook, who could move as many spaces as it wants forward but there is pawns in the path, it can not just go through and get all the pawns in its path. it has to get one at a time. it is like the pawns are barrier for it. so when it gets a pawn it can move on to the next pawn, next turn. and but knights are different. they can hop over pieces though. and the queen some people like to use it a lot. you can decide what you want to do. if you can use, it is good to use your queen a lot. but some people do not like to because it is too risky. but then you can get a lot of players if you use it. and there is no point in having it and just letting it sit there. and it is also really good against the other team's queen. so then it has nothing because it has no queen to defend the king. it is good. it does not really matter that much, pawn. if you get a pawn it is really good. it is really good if you get one of the more important pieces umm the knights, the rooks, the bishops, and the king and the queen. they are all in the back row. and the pawns go in to front row. so it is better to do that. and if something got your king in check, there is lots of things you can do. if it is possible, you can move your king so it can not get into check. but if you can not do that, if there is a piece like your queen next to it that is still in there, you can go diagonally and block it. or you can go with a knight. and you can block your you can block your king. and you can and you can block and you can block your king if you can not move anywhere with it. and then and then sort of like if it is right in front, it can be in front of the king. then it can be right in front. in front of the king and it can be anywhere in front of the king that the player can not get it. and you are not allowed to do a move that puts your king in check. that is illegal. that is against the rules in chess. so you can not do that.-->>\t0.6341827511787415\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "df[f\"{score_column}_prediction\"] = predictions\n",
    "#df.to_csv('../data/child_response_pairs.csv', index=False)\n",
    "\n",
    "#print top 10 predictions sorted by score\n",
    "df_display = df.sort_values(by=f\"{score_column}_prediction\", ascending=False).head(15)\n",
    "\n",
    "\n",
    "for index, row in df_display.iterrows():\n",
    "    prediction = row[f\"{score_column}_prediction\"]\n",
    "    print(f\"{row['text']}-->> \\t{row['CHI_response']}-->>\\t{prediction}\")\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = BayesianSentenceLengthSkewModel.load(\"../models/bayesian_sentence_length_model.model\")\n",
    "model2.predict(\"This is a nice thing to do in summer.\",temperature=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32669073286908284"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(\"This is a nice thing to do in summer.\",temperature=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6588692877481858"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(\"This is a very nice thing to do.\",temperature=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9307539992989925"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"This is a vce.\",temperature=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median length: 2.0\n",
      "True\n",
      "Left variance: 0.6800962510446344\n",
      "True\n",
      "Right variance: 10.31034952228082\n",
      "True\n",
      "Word counts: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.1176470588235294, 1.125, 1.1666666666666667, 1.2, 1.25, 1.25, 1.3333333333333333, 1.3333333333333333, 1.3333333333333333, 1.3333333333333333, 1.3333333333333333, 1.3333333333333333, 1.3333333333333333, 1.3333333333333333, 1.3333333333333333, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.6, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.6666666666666667, 1.75, 1.7647058823529411, 1.7692307692307692, 1.8, 1.8888888888888888, 1.9090909090909092, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0833333333333335, 2.1538461538461537, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2222222222222223, 2.25, 2.25, 2.25, 2.25, 2.25, 2.25, 2.2857142857142856, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.3333333333333335, 2.375, 2.4, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.6, 2.6666666666666665, 2.6666666666666665, 2.6666666666666665, 2.6666666666666665, 2.6666666666666665, 2.6666666666666665, 2.6666666666666665, 2.6666666666666665, 2.6666666666666665, 2.6666666666666665, 2.6666666666666665, 2.6666666666666665, 2.6666666666666665, 2.6666666666666665, 2.6666666666666665, 2.7142857142857144, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.8, 2.8, 2.8333333333333335, 2.8333333333333335, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.2, 3.25, 3.25, 3.25, 3.25, 3.25, 3.2857142857142856, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.3333333333333335, 3.4, 3.4, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3.6, 3.625, 3.6666666666666665, 3.6666666666666665, 3.6666666666666665, 3.6666666666666665, 3.6666666666666665, 3.6666666666666665, 3.6666666666666665, 3.6666666666666665, 3.6666666666666665, 3.6666666666666665, 3.7058823529411766, 3.75, 3.75, 3.769230769230769, 3.857142857142857, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.2, 4.25, 4.285714285714286, 4.333333333333333, 4.333333333333333, 4.333333333333333, 4.333333333333333, 4.333333333333333, 4.333333333333333, 4.333333333333333, 4.333333333333333, 4.333333333333333, 4.333333333333333, 4.333333333333333, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.571428571428571, 4.6, 4.666666666666667, 4.666666666666667, 4.666666666666667, 4.666666666666667, 4.666666666666667, 4.666666666666667, 4.666666666666667, 4.666666666666667, 4.666666666666667, 4.75, 4.75, 4.8, 4.833333333333333, 4.888888888888889, 4.923076923076923, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.25, 5.25, 5.333333333333333, 5.333333333333333, 5.333333333333333, 5.5, 5.5, 5.5, 5.5, 5.5, 5.5, 5.5, 5.5, 5.5, 5.5, 5.5, 5.5, 5.6, 5.666666666666667, 5.666666666666667, 5.666666666666667, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.166666666666667, 6.1875, 6.2, 6.333333333333333, 6.333333333333333, 6.4, 6.5, 6.5, 6.5, 6.5, 6.5, 6.5, 6.5, 6.5, 6.5, 6.5, 6.5, 6.5, 6.5, 6.5, 6.666666666666667, 6.666666666666667, 6.75, 6.75, 6.8, 6.8, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.2, 7.2, 7.333333333333333, 7.5, 7.5, 7.5, 7.5, 7.5, 7.583333333333333, 7.666666666666667, 7.666666666666667, 7.666666666666667, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.2, 8.5, 8.5, 8.8, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.333333333333334, 9.333333333333334, 9.5, 9.5, 9.5, 9.5, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.5, 10.75, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.5, 11.75, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.333333333333334, 12.5, 12.5, 12.5, 12.5, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 14.0, 14.0, 14.0, 14.0, 14.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.4, 15.5, 16.0, 17.0, 18.0, 18.0, 20.0, 27.0, 33.0]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with open(\"../models/bayesian_sentence_length_model.model\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line.startswith(\"median_length=\"):\n",
    "            median_length = float(line.split(\"=\")[1])\n",
    "            print(f\"Median length: {median_length}\")\n",
    "            print(median_length == model.median_length)\n",
    "        elif line.startswith(\"left_variance=\"):\n",
    "            left_variance = float(line.split(\"=\")[1])\n",
    "            print(f\"Left variance: {left_variance}\")\n",
    "            print(left_variance == model.left_variance)\n",
    "        elif line.startswith(\"right_variance=\"):\n",
    "            right_variance = float(line.split(\"=\")[1])\n",
    "            print(f\"Right variance: {right_variance}\")\n",
    "            print(right_variance == model.right_variance)\n",
    "        elif line.startswith(\"word_counts=\"):\n",
    "            counts_str = line.split(\"=\")[1]\n",
    "            word_counts = [float(x) for x in counts_str.split(\",\") if x]\n",
    "            print(f\"Word counts: {word_counts}\")\n",
    "            print(word_counts == model.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.31034952228082"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.right_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6800962510446344"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.left_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.median_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Childes",
   "language": "python",
   "name": "childes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
